# -*- coding: utf-8 -*-
"""Tp7_new.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11Zb0_bkAm7TWe9cS0IjhmrZu6eXfzrEc

# TP 7 - BERT pour l‚Äôanalyse de sentiment : Finetuning sur des News financi√®res
"""

!pip install transformers --upgrade #upgrade your transformers library in case it's outdated
!pip install datasets --upgrade
from datasets import load_dataset, concatenate_datasets, DatasetDict, ClassLabel
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments #Import TrainingArguments instead of TrainingArgument
from sklearn.metrics import classification_report
import numpy as np
from sklearn.metrics import accuracy_score, precision_recall_fscore_support

# Chargement des deux datasets annot√©s pour le sentiment financier
ds1 = load_dataset("zeroshot/twitter-financial-news-sentiment")
ds2 = load_dataset("nickmuchi/financial-classification")

print(ds1) #train et validation
print(ds2) #train et test

"""1. Harmonisation des labels entre les deux jeux de donn√©es"""

#On harmonise les labels des deux jeux de donn√©es d'entra√Ænement de sentiment financier provenant de sources diff√©rentes.
# Pour chaque dataset, les labels d'origine (bearish, bullish, neutral ou negative, positive, neutral) sont remapp√©s afin d'assurer une coh√©rence d'encodage entre les jeux de donn√©es.
# On applique ensuite ces fonctions de mapping √† chaque dataset, puis on uniformise le nom de la colonne cible et on supprime les colonnes redondantes si n√©cessaire.

def map_labels(example):
    # Pour ds1 :
    # Check if the key '__hf_source' exists before accessing it
    if '__hf_source' in example and "twitter-financial-news-sentiment" in str(example['__hf_source']):
        if example['label'] == 0: # Bearish
            example['label'] = 2 # Negative
        elif example['label'] == 1: # Bullish
            example['label'] = 1 # Positive
        elif example['label'] == 2: # Neutral
            example['label'] = 0 # Neutral
    return example

ds1 = ds1.map(map_labels)

def map_labels2(example):
    # Pour ds2 :
    # Check if '__hf_source' exists before accessing it
    if '__hf_source' in example and "financial-classification" in str(example['__hf_source']):
        # This block was empty, adding a pass statement to avoid syntax error
        pass
    #If '__hf_source' is not present, we assume the example is from financial-classification (since it's being applied to ds2)
    else: # This else statement was not indented correctly
        if example['labels'] == 0: # Negative
            example['labels'] = 2 # Negative
        elif example['labels'] == 1: # Positive
            example['labels'] = 1 # Positive
        elif example['labels'] == 2: # Neutral
            example['labels'] = 0 # Neutral
    return example

ds2 = ds2.map(map_labels2)


# Renommer 'labels' en 'label' dans ds2 apr√®s avoir fait la conversion :
ds2 = ds2.rename_column("labels", "label")

# Supprimer la colonne labels si elle existe :
try:
    ds1 = ds1.remove_columns("labels")
except:
    pass

# Concat√©nation des datasets
combined_dataset = DatasetDict({
    'train': concatenate_datasets([ds1['train'], ds2['train']]),
    'test': concatenate_datasets([ds1['validation'], ds2['test']])
})

print("Dataset combin√© :")
print(combined_dataset)

"""2. Fonction d'entra√Ænement, d'√©valuation et de sauvegarde d'un mod√®le de classification de texte

"""

def train_model(model_name, dataset, batch_size=16, num_epochs=3, save_path=None):
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)

    def tokenize_function(examples):
        return tokenizer(examples["text"], padding="max_length", truncation=True, max_length=128)

    tokenized_train = dataset['train'].map(tokenize_function, batched=True)
    tokenized_test = dataset['test'].map(tokenize_function, batched=True)

    tokenized_train.set_format("torch", columns=["input_ids", "attention_mask", "label"])
    tokenized_test.set_format("torch", columns=["input_ids", "attention_mask", "label"])

    training_args = TrainingArguments(
        output_dir=f"./{model_name.replace('/', '_')}_results",
        per_device_train_batch_size=batch_size,
        per_device_eval_batch_size=batch_size,
        num_train_epochs=num_epochs,
        weight_decay=0.01,
        logging_dir=f"./{model_name.replace('/', '_')}_logs",
    )

    def compute_metrics(pred):
        labels = pred.label_ids
        preds = np.argmax(pred.predictions, axis=1)
        acc = accuracy_score(labels, preds)
        precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average="weighted")
        return {"accuracy": acc, "precision": precision, "recall": recall, "f1": f1}

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_train,
        eval_dataset=tokenized_test,
        compute_metrics=compute_metrics
    )

    trainer.train()
    metrics = trainer.evaluate()

    if save_path is None:
        base_path = "/content/drive/MyDrive/model_finetuned"
        save_path = os.path.join(base_path, f"{model_name.replace('/', '_')}_finetuned")

    os.makedirs(save_path, exist_ok=True)

    print("üöÄ Sauvegarde du mod√®le dans :", save_path)
    model.save_pretrained(save_path)
    tokenizer.save_pretrained(save_path)

    print("üìÇ Contenu du dossier apr√®s sauvegarde :", os.listdir(save_path))

    print(f"‚úÖ Mod√®le '{model_name}' entra√Æn√© et sauvegard√© dans : {save_path}")
    print(f"üìä Performances : {metrics}")

    return metrics

#v√©rifier que le drive est bien mont√© sur colab
import os
print("Drive mont√© ici :", os.path.exists("/content/drive/MyDrive"))

"""3. Finetuning des mod√®les"""

# Finetuner les mod√®les BERT et FinBERT, comparer les performances et sauvegarder les poids pour le TP8.
train_model("bert-base-uncased", combined_dataset, batch_size=16, num_epochs=1)
train_model("yiyanghkust/finbert-tone", combined_dataset, batch_size=16, num_epochs=1)

"""**Interpr√©tation des r√©sultats :**

Perte (Loss) : Le mod√®le BERT a une perte d'√©valuation l√©g√®rement plus faible (0.3566) par rapport √† FinBERT (0.3591), ce qui indique qu‚Äôil a mieux appris √† minimiser l'erreur lors de l'entra√Ænement.

Accuracy : Le mod√®le BERT obtient une pr√©cision de 86.45%, tandis que FinBERT a une pr√©cision de 85.90%. Cela montre que BERT fait l√©g√®rement plus de pr√©dictions correctes que FinBERT.

Pr√©cision : BERT a une pr√©cision de 86.31%, qui est √©galement un peu meilleure que celle de FinBERT (86.12%). Cela signifie que BERT a un peu moins de faux positifs que FinBERT.

Rappel (Recall) : Le rappel de BERT est aussi 86.45%, ce qui est sup√©rieur √† celui de FinBERT qui est de 85.90%. Cela montre que BERT a mieux r√©ussi √† capturer les cas positifs que FinBERT.

Score F1 : Le score F1 de BERT est de 86.31%, tandis que celui de FinBERT est de 85.99%. Le score F1 combine la pr√©cision et le rappel, et il est √©galement un peu meilleur pour BERT.

Temps d'ex√©cution (Runtime) : Les temps d'ex√©cution sont tr√®s similaires, avec BERT prenant 4.96 secondes et FinBERT prenant 4.99 secondes, ce qui montre que les deux mod√®les sont assez rapides.

Nombre d'exemples trait√©s par seconde : BERT traite 583 exemples par seconde, tandis que FinBERT traite 580 exemples par seconde. L√† aussi, les performances sont tr√®s proches.

Conclusion : Le mod√®le BERT surpasse l√©g√®rement FinBERT sur toutes les m√©triques de performance. Bien que les diff√©rences soient minimes, BERT semble √™tre plus efficace pour cette t√¢che d'√©valuation. Toutefois, FinBERT, √©tant un mod√®le sp√©cialis√© pour le domaine financier, peut avoir des avantages pour des cas sp√©cifiques li√©s √† la finance.
"""