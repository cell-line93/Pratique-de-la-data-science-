# -*- coding: utf-8 -*-
"""TP 2 last.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10S2VecmEhpjHcVnpYpMIPeTxEv8U_uPA

# 1 TP 2 - Clustering
"""

import yfinance as yf
import pandas as pd
from datetime import datetime
import numpy as np

from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, SpectralClustering
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.manifold import TSNE
from scipy.spatial.distance import cdist
from scipy.cluster.hierarchy import linkage, dendrogram, fcluster
from scipy.spatial.distance import squareform, pdist
import glob

"""# 1.1 Financial profiles clustering

Objectif : Identifier les groupes d’entreprises aux caractéristiques financières similaires (Valorisation, performances financière...) =⇒ Peut permettre de créer des portefeuilles diversifiés.

Algorithme : K-MEANS.

Pre processing: def preprocess for financial clustering()

1. Charger les ratios financiers obtenus grâce au scrapping dans un dataframe.
"""

ratios_pd = pd.read_csv('ratios.csv')

"""2. Extraire dans un nouveau dataframe les données pertinentes pour une analyse de performances financières (ForwardPE, Beta, Price to Book, Return on equity....)

3. Nettoyer les NA (Basique: on supprime les entreprises qui présentent des NA sur ces données).

4. Standardiser les données.
"""

def preprocess_for_clustering(data, category="finance"):
    if category == "finance":
        selected_columns = [
            "forwardPE", "beta", "priceToBook", "returnOnEquity",
            "returnOnAssets", "operatingMargins", "profitMargins",
            "currentRatio", "quickRatio", "debtToEquity"
        ]
    elif category == "risque":
        selected_columns = [
         "debtToEquity","currentRatio","quickRatio","dividendYield","returnOnEquity",
         "returnOnAssets","operatingMargins","profitMargins"
         ]
    else:
        raise ValueError(f"Catégorie inconnue : {category}")

    selected_data = data[selected_columns].dropna()
    scaled_data = StandardScaler().fit_transform(selected_data)

    return scaled_data, selected_data

ratios_finance_std, ratios_finance = preprocess_for_clustering(ratios_pd, category = "finance")

"""Déterminer le nombre de clusters: def elbow method(data)

1. Utiliser KMeans en faisant varier le nombre de clusters K.

2. Analyser l’inertie en fonction de K (.inertia ).

3. Plot les inerties et trouver le ”coude”.
"""

def elbow_method(data,nb_cluster):
    inerties = []

    for k in range(1,nb_cluster):
        kmeans = KMeans(n_clusters = k, random_state = 42)
        kmeans.fit(data)
        inerties.append(kmeans.inertia_)
    plt.figure(figsize=(8, 5))
    plt.plot(range(1, 25), inerties, marker='o', linestyle='--')
    plt.title('Elbow method')
    plt.xlabel('Number of cluster (k)')
    plt.ylabel('Inertia')
    plt.grid(True)
    plt.show()
    return inerties

elbow_method(ratios_finance_std,25)

"""On peut estimer le coude à K = 8

1. Appliquer Kmeans avec nombre de clusters décidé.

2. Rajouter la colonne ”clusters” au dataframe.

3. Analyser les caractéristiques de chaque cluster et plot une représentation TSNE (Entreprises color ́ees par clusters).
"""

def do_kmeans_clustering(data_scaled, data, nb_cluster):
    #We retransform it into a dataframe, (it became an array when we standardized the data)
    data_df = pd.DataFrame(data_scaled, columns = data.columns)
    #We apply kmeans with k chosen
    km = KMeans(n_clusters = nb_cluster, random_state = 42)
    km.fit(data_scaled)
    labels = km.fit_predict(data_scaled)
    #We add the column "clusters" to our dataframe
    data_df["clusters"] = labels


    tsne = TSNE(n_components=2, random_state=42,perplexity=min(30, len(data_scaled) - 1))
    data_tsne = tsne.fit_transform(data_scaled)

    #plotting
    plt.figure(figsize=(8, 6))
    scatter = plt.scatter(data_tsne[:, 0], data_tsne[:, 1], c=labels, cmap='tab10', s=50, alpha=0.7)
    plt.colorbar(scatter, label='Cluster')
    plt.title('Clusters visualization with t-SNE')
    plt.xlabel('t-SNE Dimension 1')
    plt.ylabel('t-SNE Dimension 2')
    plt.grid(True)
    plt.show()

    return(data_df)

do_kmeans_clustering(ratios_finance_std, ratios_finance, 8)

"""# 1.2 Risk profiles clustering

Objectif : Identifier les groupes d’entreprises aux profils de risques (financiers et opérationnels) similaires (endettement, liquidité, rentabilité...) =⇒ Peut permettre d’ajuster les allocations d’actifs.


Algorithme : Hierarchical Clustering.

Hierchical clustering (def do hierarchical clustering()) et visualisation :
1. Appliquer hierarchical clustering (AgglomerativeClustering(...)
2. Rajouter colonne ”clusters” et analyser charact ́eristiques.
3. Plot le dendogram (def plot dendrogram())
• Calcul des distances (linked = linkage(data, method=’ward’) • plot ces distances dendogramm(...)
"""

ratios_risk_std,ratios_risk = preprocess_for_clustering(ratios_pd, category = "risque")

def do_hierarchical_clustering(data, data_init,  nb_cluster):
    data = pd.DataFrame(data, columns = data_init.columns)
    agg = AgglomerativeClustering(n_clusters=nb_cluster)
    labels_2 = agg.fit_predict(data)
    data["clusters"] = labels_2
    return data

def plot_dendrogram(data,data_init):
    #data = pd.DataFrame(data, columns = data_init.columns)
    linked = linkage(data, method='ward')
    plt.figure(figsize=(10, 6))
    dendrogram(linked,orientation='top',
           distance_sort='descending',
          #labels=data_init.columns,
           show_leaf_counts=True)
    plt.title('Hierarchical clustering Dendrogramme')
    plt.tight_layout()
    plt.show()

plot_dendrogram(ratios_risk_std,ratios_risk)

"""# 1.3 Daily returns correlations clustering

Objectif : Regrouper les entreprises en fonction de la similarité de leurs  evolutions boursières =⇒ Peut permettre de mieux diversifier son portefeuille.


Algorithme : Hierarchical Clustering.

**Préparation des données (Rendements des entreprises):**
1. Créer un dictionnaire (vide) qui va contenir les rendements de chaque entreprise.
2. Pour chaque entreprise, charger la colonne ”Daily Return” et l’ajouter au dictionnaire (clé :
company name, valeur : colonne daily return).
3. Transformer le dictionnaire en dataframe ”returns df”.
4. Nettoyer les valeurs manquantes (Basique: avec moyenne par exemple).
"""

import zipfile
import os

def preprocess_for_return_clustering(zip_filepath):
    Rend_dict = {}

    # Générer un nom de dossier basé sur le nom du fichier zip (sans extension)
    extract_dir = os.path.splitext(zip_filepath)[0]

    # Extraire les fichiers du zip
    with zipfile.ZipFile(zip_filepath, 'r') as zip_ref:
        zip_ref.extractall(extract_dir)

    # Obtenir la liste des fichiers extraits
    filepaths = glob.glob(os.path.join(extract_dir, "*"))

    # Lire les rendements de chaque fichier CSV
    for filepath in filepaths:
        company_name = os.path.basename(filepath)
        Rend_dict[company_name] = pd.read_csv(filepath)["Rendement"]

    # Créer un DataFrame avec les rendements
    return_df = pd.DataFrame(Rend_dict)

    # Remplir les valeurs manquantes avec la moyenne de chaque colonne
    return_df = return_df.fillna(return_df.mean())

    return return_df

return_df = preprocess_for_return_clustering('Companies_historical_data.zip')

"""
**Hierachical clustering:**
1. Extraire les corrélations entre les rendements des entreprises (returns df.corr())
2. Appliquer le hierachical clustering sur cette matrice de corrélations et plot le dendogram."""

corr_matrix = return_df.corr()

#On convertit la matrice de corrélation en une matrice de distance
#Forte corrélation = faible distance

distance_matrix = 1 - corr_matrix.abs()

#Convertion en vecteur pour pouvoir utiliser linkage

condensed_distance = squareform(distance_matrix, checks=False)

plot_dendrogram(condensed_distance,condensed_distance)

"""# 1.4 Evaluation et comparaison des algorithmes

1. Implémenter l’algorithme DBSCAN (def do hierarchical clustering()).
"""

def DBSCAN_algorithm(data):
  #normalisation
  data_scaled = StandardScaler().fit_transform(data)

  #DBSCAN
  db = DBSCAN(eps=0.8, min_samples=3)
  labels = db.fit_predict(data_scaled)



  return(data_scaled,labels)

"""2. Utiliser les trois algorithmes (KMEANS, Hierarchical, DBSCAN) sur les trois critères (finance, risk, rendements).

"""

def apply_all_clustering_algorithms(finance, risk, rendements):
    subsets = {'Finance': finance, 'Risk': risk, 'Rendements': rendements}

    for name, subset in subsets.items():
        print(f"\n==== {name.upper()} CRITÈRE ====\n")

        print(">>> KMEANS")
        data_scaled = StandardScaler().fit_transform(subset)
        do_kmeans_clustering(data_scaled, subset, nb_cluster=3)

        print(">>> HIERARCHICAL")
        data_scaled = StandardScaler().fit_transform(subset)
        plot_dendrogram(data_scaled, subset)

        print(">>> DBSCAN")
        data_scaled, labels = DBSCAN_algorithm(subset)
        plt.figure(figsize=(8, 6))
        tsne = TSNE(n_components=2, random_state=42,perplexity=min(30, len(data_scaled) - 1))
        data_tsne = tsne.fit_transform(data_scaled)
        scatter = plt.scatter(data_tsne[:, 0], data_tsne[:, 1], c=labels, cmap='tab10', s=50, alpha=0.7)
        plt.colorbar(scatter, label='Cluster')
        plt.title('DBSCAN Clustering with t-SNE')
        plt.xlabel('t-SNE Dimension 1')
        plt.ylabel('t-SNE Dimension 2')
        plt.grid(True)
        plt.show()

apply_all_clustering_algorithms(ratios_finance, ratios_risk, return_df)

"""3. Silhouette score: Fournir un tableau avec silhouette score pour chaque algorithme sur chaque type de données. Analyser."""

from sklearn.metrics import silhouette_score

def compute_silhouette_scores(finance, risk, rendements):
    from sklearn.cluster import AgglomerativeClustering

    datasets = {'Finance': finance, 'Risk': risk, 'Rendements': rendements}
    algorithms = ['KMeans', 'Hierarchical', 'DBSCAN']
    results = []

    for name, data in datasets.items():
        data_scaled = StandardScaler().fit_transform(data)

        # KMeans
        kmeans = KMeans(n_clusters=3, random_state=42)
        kmeans_labels = kmeans.fit_predict(data_scaled)
        kmeans_score = silhouette_score(data_scaled, kmeans_labels)

        # Hierarchical
        hier = AgglomerativeClustering(n_clusters=3)
        hier_labels = hier.fit_predict(data_scaled)
        hier_score = silhouette_score(data_scaled, hier_labels)

        # DBSCAN
        db = DBSCAN(eps=0.8, min_samples=3)
        db_labels = db.fit_predict(data_scaled)
        # Vérifier s'il y a au moins 2 clusters (silhouette_score ne fonctionne pas sinon)
        if len(set(db_labels)) > 1 and -1 in db_labels:
            db_score = silhouette_score(data_scaled, db_labels)
        else:
            db_score = None  # Pas possible de calculer

        # Stocker
        results.append({'Dataset': name, 'Algorithm': 'KMeans', 'Silhouette Score': kmeans_score})
        results.append({'Dataset': name, 'Algorithm': 'Hierarchical', 'Silhouette Score': hier_score})
        results.append({'Dataset': name, 'Algorithm': 'DBSCAN', 'Silhouette Score': db_score})

    return pd.DataFrame(results)

scores_df = compute_silhouette_scores(ratios_finance, ratios_risk, return_df)
print(scores_df)